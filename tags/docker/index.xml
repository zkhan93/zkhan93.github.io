<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>docker on Zeeshan Khan</title><link>https://zkhan.in/tags/docker/</link><description>Recent content in docker on Zeeshan Khan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 21 Mar 2023 15:32:23 -0500</lastBuildDate><atom:link href="https://zkhan.in/tags/docker/index.xml" rel="self" type="application/rss+xml"/><item><title>Elasticsearch Kibana Setup</title><link>https://zkhan.in/posts/elasticsearch-kibana-setup/</link><pubDate>Tue, 21 Mar 2023 15:32:23 -0500</pubDate><guid>https://zkhan.in/posts/elasticsearch-kibana-setup/</guid><description>I was trying to setup a elasticsearch with docker compose to index files on my NAS(Samba) and I had to struggle a bit to get that up and running, this post outlines the steps needed to setup elasticsearch with docker compose
All my self-hosted services are behind traefik proxy to manage SSL certificates and I have chosen to keep communication between traefik and elasticsearch to HTTP only, so I have disabled SSL security in elasticsearch.</description></item><item><title>Duplicate File Finder Web Based Solution</title><link>https://zkhan.in/posts/duplicate-file-finder/</link><pubDate>Tue, 27 Sep 2022 23:22:29 -0500</pubDate><guid>https://zkhan.in/posts/duplicate-file-finder/</guid><description>I was trying to backup my home server when I saw my sister&amp;rsquo;s folder had around 300GB worth of files, and since she had migrated/upgraded to different machines over time and finally moved all the data over to the home server. I knew some(~50GB) of that would be duplicate files.
So on a weekend, I decide to clean up some of that duplicate data before backing it up and start paying for it.</description></item></channel></rss>